import torch
import os, sys, json, time, warnings
import numpy as np
import pandas as pd
from tqdm import tqdm

from config import CONFIG, device, SEED
from utils import load_data, load_models, save_metrics_csv
from evaluations import evaluate_epsilon_sensitivity 
from attacks import torchattacks

def main_epsilon_sensitivity_script():
    print("\n" + "="*60 + "\n Epsilon Sensitivity Analysis Script \n" + "="*60)
    print(f"Epsilon sensitivity results will be saved in subdirectories under: {CONFIG['results_base_dir']}")

    # Load Data & Models
    val_loader_eps = load_data(CONFIG)
    models_collection_eps = load_models(CONFIG)
    if not models_collection_eps:
        print("CRITICAL ERROR: No models loaded. Epsilon sensitivity analysis cannot proceed."); sys.exit(1)

    # Select the model for epsilon sensitivity tests
    primary_model_key_eps = 'resnet50_clean' 
    if primary_model_key_eps not in models_collection_eps:
        primary_model_key_eps = next(iter(models_collection_eps))
    model_for_eps_test = models_collection_eps[primary_model_key_eps]
    print(f"Using model for epsilon sensitivity analysis: {primary_model_key_eps}")

    # Define Attacks for Epsilon Sensitivity Test
    attacks_for_eps_analysis = {}
    
    pgd_base_config_eps = {
        'alpha': CONFIG['alpha'],
        'steps': CONFIG['steps'],
        'random_start': True
    }
    attacks_for_eps_analysis['PGD_Epsilon_Sweep'] = {
        'class': torchattacks.PGD,
        'base_params': pgd_base_config_eps 
    }

    print("\nAttacks selected for Epsilon Sensitivity Analysis:")
    if not attacks_for_eps_analysis: 
        print("Warning: No attacks defined for epsilon sensitivity. Script will exit."); return
    for attack_id_eps in attacks_for_eps_analysis: 
        print(f" - Scheduled: {attack_id_eps}")

    epsilon_test_range = CONFIG['epsilon_range']
    print(f"Epsilon values to test: {epsilon_test_range}")

    # Store all epsilon sensitivity summaries
    master_epsilon_sensitivity_reports = {}

    # Run Epsilon Sensitivity Evaluation Loop
    for attack_id_key_eps, attack_config_eps in attacks_for_eps_analysis.items():
        print(f"\n--- Running Epsilon Sensitivity Analysis for: {attack_id_key_eps} ---")
        attack_eps_results_dir = os.path.join(CONFIG['results_base_dir'], attack_id_key_eps)
        os.makedirs(attack_eps_results_dir, exist_ok=True)

        current_eps_sensitivity_summary = evaluate_epsilon_sensitivity(
            model_for_eps_test, 
            attack_config_eps['class'], 
            val_loader_eps,
            epsilon_test_range, 
            attack_config_eps['base_params'],
            attack_id_key_eps,
            CONFIG
        )
        master_epsilon_sensitivity_reports[attack_id_key_eps] = current_eps_sensitivity_summary
        
        summary_file_path_eps = os.path.join(attack_eps_results_dir, f'summary_epsilon_sensitivity_{attack_id_key_eps}.json')
        with open(summary_file_path_eps, 'w') as f_json_eps:
            json.dump(current_eps_sensitivity_summary, f_json_eps, indent=4, default=lambda o: o.item() if hasattr(o, 'item') else str(o))
        print(f"Saved epsilon sensitivity summary for {attack_id_key_eps} to: {summary_file_path_eps}")

    # --- Save an Overall Master Summary for all Epsilon Tests Conducted ---
    master_eps_summary_file_path = os.path.join(CONFIG['results_base_dir'], 'MASTER_EPSILON_SENSITIVITY_REPORT.json')
    with open(master_eps_summary_file_path, 'w') as f_master_eps_json:
        json.dump(master_epsilon_sensitivity_reports, f_master_eps_json, indent=4, default=lambda o: o.item() if hasattr(o, 'item') else str(o))
    
    print(f"\nAll epsilon sensitivity analyses complete. Master report saved to: {master_eps_summary_file_path}")
    print(f"Detailed CSVs (if generated by evaluation function) and individual JSON summaries are in subdirectories under: {CONFIG['results_base_dir']}")

if __name__ == "__main__":
    warnings.filterwarnings("ignore") # General ignore for cleaner output, be specific if needed
    main_epsilon_sensitivity_script()

